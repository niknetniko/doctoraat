%! Author = niko
%! Date = 23/10/2023

% Preamble
\documentclass[../main]{subfiles}
\usepackage{glossaries}

% Document
\begin{document}

\chapter{The \tested{} test engine}
\label{ch:tested-deel-1-tijdelijke-titel}

This chapter describes the internal mechanisms and workings of TESTed.
We begin with an overview of the ``architectural'' or conceptual design.
This is followed by a detailed look at the evaluation process.
The evaluation of a submission is the process through which TESTed executes the test suite and determines the feedback that will be given on the submission.
Finally, we look at how support for multiple programming languages is achieved.

\section{Conceptual design}
\label{sec:conceptual design}

\begin{figure}[t]
    \centering
    \includestandalone{concept}
    \caption{
        Conceptual design of TESTed, with colors indicating different programming languages.
        The framework consists of a set of Python packages and modules.
        These can be categorized as the core package and a set of programming-language-specific modules.
        The input for TESTed consists of a test suite, together with a submitted solution in one of the supported languages
        The output is the generated feedback.
    }
    \label{fig:conceptual-design}
\end{figure}

The main idea behind a programming-language-independent test framework is that an exercise designer can write a single test suite for a programming exercise, while the test framework is still able to evaluate submissions in multiple programming languages.

TESTed implements this concept using code generation.
This means that TESTed converts the test suite on the fly suite into the programming language of the submission.
It also takes care of the various aspects of the evaluation process: compiling the submitted code, executing the submission together with the test code, interpreting the results, and generating feedback.

While some parts of the evaluation process are programming-language-specific by necessity, such as generating the test code, a lot of parts are not.
For example, creating an execution plan or interpreting the test results and generating the feedback are not specific to any one programming language.
Therefore, the language-specific aspects are isolated in language modules, as illustrated in~\cref{fig:conceptual-design}.

\leavevmode\marginnote{
    A Python module is a \texttt{.py} file, while a Python package is a folder containing modules.
}%
TESTed is written in Python and organized into a set of Python modules and packages.
An import package is the \emph{core} package, which contains modules that are responsible for all language-independent tasks, such as scheduling tests.
This is discussed in \cref{subsec:execution-planning}.
In most cases, the core is also responsible for checking the collected test results and generating the feedback.
This is discussed in \cref{subsec:checking-results}.

All aspects that are specific to one programming language are bundled in one package.
These language-specific modules take care of all language-specific tasks, such as compiling submissions, executing submissions, and handling language-specific data types, expressions, and statements.
These modules are discussed in \cref{sec:programming-language-support}.

Since the language-specific code is limited to these modules, this offers benefits for adding support for new programming languages to TESTed, see TODO.

TESTed requires a test suite for a programming exercise and a possible solution that needs to be evaluated as input.
The structure of the test suite is discussed in-depth in TODO.
As a result of its evaluation, TESTed outputs a feedback report as has been illustrated with some examples in the previous section. TODO


\section{Test suite structure}
\label{sec:test-suite-structure}

A test suite for TESTed is a hierarchical structure with three levels:

\marginnote{
    Since TESTed was originally developed for use with Dodona, these levels are equivalent to the Dodona levels tab, contexts and testcases (which contain tests).
}

\begin{enumerate}
    \item Units are the top-level grouping mechanism.
          It allows grouping of logically related testcases together.
    \item Testcases are the basic building blocks of a test suite.
          A testcase is a set of dependent tests.
          Testcases are independent of each other.
    \item Tests are the lowest level.
          A test consists of some input and a series of checks about different side effects or results (i.e.\ return values, standard out, standard error, exit codes or exceptions).
          Each output check is also called a script (since the input together with the output checks creates the script of a test).
\end{enumerate}

This structure mirrors the output generated by TESTed.
For example, the executed input for each test is also included in the output.
A possible visualization of these levels is given in~\cref{fig:dodona}, which shows the output rendered in Dodona.
TODO: reference

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{dodona-rendering}
    \caption{A way to visually render the feedback (as done in Dodona resulting from evaluating a submission (in Python) with the test suite from TODO. There are four levels: \CircledText{1} units, \CircledText{2} testcases, \CircledText{3} tests and \CircledText{4} the scripts. Here, each testcase consists of two tests, the first of which has no script, while the second has one script (the expected return value).}
    \label{fig:dodona}
\end{figure}

TODO: add dataserialisation somewhere, perhaps own chapter

\section{Evaluating submissions}\label{sec:evaluating-submissions}

The input for the evaluation process is a test suite and a submission, which is typically provided by the judge platform in which TESTed runs (or can be provided manually if TESTed is run on the command line).
A general overview is given in~\cref{subsec:evaluation-process}.
Other subsections dive into more detail of individual parts of the process.

\subsection{The complete evaluation process}\label{subsec:evaluation-process}

We can now take a look at the complete evaluation process used by TESTed when evaluating a submission (a schematic is~\cref{fig:flow}).
The very first step in the evaluation process is checking if the exercise is solvable in the programming language of the submission (see~\cref{subsec:solvability-and-correctness-checks}).
Next, in the first step of the actual evaluation, the test suite is partitioned into compilation and execution units, as described in~\cref{subsec:execution-planning}.
For each of these units, the relevant code is generated and the resulting test code is compiled, as explained in~\cref{subsec:code-generation}.
The code is generated in the programming language of the submission.
This code generation is the bulk of language-specific code in TESTed, whose design and implementation are discussed in Programming-language-specific modules (TODO).

After compilation, each resulting executable contains one or more execution units.
These are then all executed, and the side effects (such as exceptions, stdout, stderr, etc.) and results (i.e.\ return values) are recorded.
These results are then checked for correctness against the test suite (see TODO).
This results in the feedback, which is returned by TESTed.

TESTed also has a step where static analysis is possible on the submission.
\marginnote{Examples include \texttt{ESLint}, \texttt{pylint} or \texttt{hlint}.}
Currently, all supported programming languages use this step to run a linter on the submission, the results of which are also included in the feedback as code annotations. SEE X.

\begin{figure}
    \centering
    \includestandalone{flow}
    \caption{
        The evaluation process of TESTed.
        The input consists of a submission and a test suite.
        After the planning phase, test code is generated, compiled, and executed.
        These results are then checked, which produces the final feedback.
        Separately, a linter runs on the submission, and its results are also included in the feedback.
    }
    \label{fig:flow}
\end{figure}

\subsection{Solvability and correctness checks}\label{subsec:solvability-and-correctness-checks}

The first step in the evaluation process is checking if the test suite is usable for the programming language of the submission.
This might not be the case for a number of reasons, the three main ones being:

\begin{itemize}
    \item The exercise designer has limited in which programming languages the exercise may be solved.
    \item The test suite uses constructs that are not supported by the programming language of the solution.
          For example, if the test suite uses object-oriented programming, the exercise will not be solvable in C or Haskell.
    \item The test suite contains programming-language-specific code but does not provide code for the programming language of the submission.
          For example, it is possible to manually provide the test code.
\end{itemize}

Additionally, there are some correctness checks, for example, on the syntax of the test suite.

\subsection{Execution planning}\label{subsec:execution-planning}

The next step is planning the execution of the evaluation.
As discussed before, a test suite contains a number of testcases, which must be independent of each other.
This allows TESTed to implement optimisations to improve performance.

TESTed partitions the test suite into compilation units (a set of testcases that are compiled together), which are in turn partitioned into execution units (a set of testcases that are executed together).
While different compilation units cannot be executed as one execution unit, it is possible that one compilation unit represents multiple execution units.

\begin{figure}
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone{planning-1}
        \caption{
            A schematic representation of the test suite.
            The test units are represented with green boxes, while the testcases (denoted as C\textsubscript{$n$}) are black boxes.
            In the subsequent figures, we leave the test suite box out to simplify the image.
        }
        \label{fig:planning-suite}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone{planning-2}
        \caption{
            The two possibilities for compilation units (denoted by red boxes).
            The upper scheme (with one compilation for the whole test suite) is always tried first.
            If this fails, each test unit becomes a compilation unit (the red boxes thus overlap with the green ones in the figure).
        }
        \label{fig:planning-compilation}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}{\textwidth}
        \centering
        \includestandalone{planning-3}
        \caption{
            Two possibilites execution unit partitionings (denoted by blue boxes), depending on the compilation units.
            In the first paritioning, there is a single compilation unit that is split into three execution units.
            The second partitioning cannot use the same execution units, as an execution unit cannot comprise multiple compilation units.
        }
        \label{fig:planning-execution}
    \end{subfigure}
    \caption{Schematic representation of the planning steps.}
\end{figure}

\subsubsection{Performance impact of the planning}

If performance was not relevant, the easiest execution plan would be to compile and execute each testcase individually.
After all, they are independent of each other, and separate compilation and execution would ensure that independence.

However, this would be prohibitively slow: a test suite with fifty test cases would need fifty compilation steps and fifty execution steps.
Creating an execution plan is thus intended to improve performance, which is done in two ways:

\marginnote{The final execution units are also executed in parallel, which is described in TODO.}
\begin{enumerate}
    \item Reducing the number of compilation units.
    \item Reducing the number of execution units.
\end{enumerate}

\subsubsection{Compilation units}
\label{subsubsec:compilation-units}

First, TESTed tries to use a single compilation unit for the whole test suite.
This is achieved by creating a program that accepts an argument to indicate the execution that should be run.

In programming languages without an explicit compilation step, the compilation is no more than a syntax check.
\marginnote{
    For example, our JavaScript implementation uses \texttt{node -c}.
}
In compiled languages, the compilation is often much stricter, for example, failing if a non-existing function is used.

Techniques used to improve performance must be weighed against the usability.
For example, consider an exercise where students must implement two or three functions.
Students might often implement the first function and submit their solution.
With a single compilation unit, a compilation would occur, since our test code calls functions that do not exit.
This will prevent the first function from being evaluated, even if it was correct.

To prevent this, if the compilation of the whole test suite fails, TESTed falls back to using one compilation unit per unit in the test suite.
These two approaches are illustrated in~\cref{fig:planning-compilation}.

This is why a unit is intended to be a set of logically related testcases.
Continuing with the example from before, there might be a separate unit for each function the exercise requires.
Going more fine-grained, that is compiling each testcase individually, does not seem useful.
While potentially providing faster feedback, the performance impact is not worth it.

Compiling at the unit level is a good compromise between fain-grained compilation (thus allowing more of the submission to be evaluated) and performance (the more compilation units, the slower the evaluation will be).

\subsubsection{Execution units}

Next, the compilation units from the previous steps are partitioned into execution units.
\marginnote{
All programming languages, including the non-compiled ones, are handled the same way.
This allows the most consistency between programming languages.
}
Each execution unit can be at most one compilation unit: we cannot execute multiple compilation units together,
as each compilation unit results in a separate executable.
However, depending on the testcases inside a compilation unit, we can (and do) execute a compilation unit multiple times.

For performance reasons, the ideal partitioning would be a single execution unit for the whole test suite.
However, this prevents certain types of exercises from being evaluated correctly.
Therefore, a new execution unit is started based on the type of testcase: if a testcase has stdin, command line arguments, or an explicit check for the exit code, a new execution unit is started.

\subsection{Generating test code}\label{subsec:code-generation}

Depending on the execution plan, the appropriate code is generated for the test suite.
In concept, this step converts the programming-language-independent test suite into actual source code, in the programming language of the test suite.
For example, for a submission in JavaScript, the generated code will also be in JavaScript.

As shown in~\cref{fig:flow}, code is generated for each compilation unit.
The main purpose of the generated code is to execute the tests specified in the test suite.
For this reason, we also call the generated code the \emph{test code}.

Besides executing the tests from the test suite, the generated code also contains some \emph{ceremonial} code required by TESTed.
Some examples of this code are:

\begin{itemize}
    \item As mentioned in~\cref{subsec:execution-planning}, in the ideal case, we only have one compilation unit and thus one executable for the whole test suite.
     We thus generate a wrapper that executes the correct execution unit based on some parameter (e.g.\ \texttt{./testcode "unit1"}).
    \item The generated code includes serialization capabilities (see TODO), to convert captured values into the internal data format used by TESTed.
    \item The tests are wrapped in code that captures side effects and values, such as exceptions, return values, etc.
\end{itemize}

This code generation is the main task of the programming-language-specific modules, discussed in TODO.

\subsection{Executing test code}\label{subsec:executing-test-code}

After the test code has been generated and compiled into executables, these executables are then (logically) executed.

During this execution, special care is taken to ensure that the executions are independent of each other.
As discussed in TODO, execution happens in a special directory called the \emph{workdir} (from working directory).
For each execution, TESTed creates a new subdirectory and copies the relevant files into that subdirectory.
The execution then happens in the subdirectory.

The independence of execution is useful to prevent other executions from interfering.
For example, consider an erroneous submission for an exercise where data is provided in a file.
If the submission overwrites or changes the file by accident, subsequent executions would use the modified file.
However, since the file is copied into each subdirectory, this scenario is prevented.

A second benefit of the independent executions is the ability to execute in parallel.
This option is disabled by default, due to the way Dodona works (TODO link).
However, even in that scenario, this improves the performance of certain exercises, for example those exercises that use scripts, command lines, or main functions.

The astute reader might wonder how this is implemented, since TESTed is written in Python, a language whose default implementation (CPython) has an infamous global interpreter lock (or GIL).
This means that at any one time, only one thread can execute Python code.
\marginnote{
Work is ongoing to improve the situation, or even remove the GIL, as PEP 703: \url{https://peps.python.org/pep-0703/}
}
One workaround is to use multiple processes: this sidesteps the issue entirely, but using multiple processes is fairly heavy in general.
This is even truer in TESTed, as executing the test code already happens in a separate process.

This leads directly to the solution in TESTed: since the test code is executed in a separate process, the GIL does not apply.
Most of the time is spent waiting on results from a subproces.

\subsection{Checking test results}\label{subsec:checking-results}

The code responsible for checking wheter test results (return values, stdout, etc.) are correct is called an oracle. TODO referentie.
TESTed supports three types of oracles, which can be divided into two categories.
The first two oracle types are generic, meaning that they are programming language independent.
These are also explained in TODO, where the focus lies on using the oracles.
The third oracle type is a programming-language-dependent oracle.

The two generic oracles are the built-in oracles and the programmed oracles.
The first are oracles are, as the name implies, built into TESTed.
These are rather simple, but should suffice for most exercises.
Currently, the following oracles are included:

\begin{itemize}
    \item The text oracle.
          This oracle compares two strings and is used for stdout and stderr.
          The oracle has some options, for example, to ignore trailing whitespace, to attempt to parse the text as floating point numbers or case sensitivity.
          The expected value can be provided either as a string, or as a file, in which case the contents of said file are used as the string.
    \item The file oracle.
          This allows comparison between two files, either comparing the whole file, or comparing the file line-by-line.
          When comparing line-by-line, the text oracle is used, so the same options can be provided.
    \item The return value oracle.
          This oracle compares two values (see TODO for the test values).
          The oracle does more than just a value comparison: the types of the data are also compared, using the TESTed type system (see TODO for how this works).
    \item The exception oracle.
          This allows checking exceptions.
          By default, only the message of the exception is checked, as the type of an exception is programming language dependent.
          However, the oracle does have an option to provide the expected type for the different programming languages, in which case those will be checked as well.
          Do note that checking the type happens with a string-based check, meaning that if a student implements a custom exception with the same name, it will pass the check.
\end{itemize}

There are a number of scenarios where the built-in oracles are not enough to properly check an exercise.
One such example is an exercise where dynamic data is used, for example a return value that depends on the current date.
Another example is a random or otherwise nondeterministic return value, where the value must satisfy some conditions.
Finally, sometimes the exercise is static, but you want to provide some exercise-specific feedback.
For example, if the exercise is to generate an SVG, you might want to show that SVG to students.

For these scenarios, a programmed oracle can be used.
Such an oracle is, in essence, an extension of the built-in oracles: instead of using a built-in oracle, TESTed will call the exercise-provided oracle.
See TODO for how to use such an oracle.
TODO: zeggen dat ze enkel in Python bestaan?

We have not yet encountered an exercise not covered by the generic oracles; however, it is not hard to imagine exercises that cannot be checked using those.
As such, TESTed provides an escape hatch: a programming-language-specific oracle.
These oracles are run together with the generated test code and skip the serialization pipeline.
They are therefore not subject to the serialization limits of TESTed.
An example could be an exercise where a function returns an instance of a custom class.

The big downside to using these oracles is that they are programming-language-specific.
The oracle thus has to be implemented in each programming language the exercise supports.
These oracles are therefore only provided as an escape route for language-specific expressions, statements, and data types.

\subsection{Static analysis of the submission}\label{subsec:static-analysis-of-the-submission}

TESTed also provides a way to perform exercise-independent static analysis on the submission as part of the evaluation process.
TODO: references naar linters en dat ze goed werken.
Currently, all supported languages (except C\#, where the compiler acts a linter) use an external linter to generate additional feedback, see~\cref{tab:linters}.
While this feedback is generally not specific to an exercise, nor is its main goal to check the correctness of the submission, it does provide additional hints to students on how to improve their submission.

TODO: refereren waar mogelijk
\begin{table}[h]
    \centering
    \caption{Overview of the used linters in TESTed.}
    \label{tab:linters}
    \begin{tabular}{|l|l|}
        \hline
        Programming language & Linter \\
        \hline
        Bash & Shellcheck  \\
        C & Cppcheck \\
        Haskell & HLint \\
        Java & Checkstyle \\
        JavaScript & ESLint \\
        Kotlin & Ktlint \\
        Python & Pylint \\
        \hline
    \end{tabular}
\end{table}

\subsection{Feedback}\label{subsec:feedback}

The last step of the evaluation process is to return the generated feedback.
The feedback format used by TESTed is the Dodona feedback format, which we discuss in \cref{subsec:dodona-output}.

\section{Programming language support}\label{sec:programming-language-support}

The parts of the evaluation process that are programming-language-specific are implemented using a module system.
To make adding new programming languages easy, TESTed enforces a strict separation of concerns with regard to programming-language-specific tasks.
All programming-language-specific actions and tasks must go through a single well-defined interface.
This interface is implemented using Python's object-oriented capabilities by defining an abstract base class (ABC) called \mintinline{python}{Language}.

This \mintinline{python}{Language} class has a set of abstract methods, for which an implementation is thus necessary, and the remaining optional methods, which may be overridden but are not required.
This abstract class is the interface between the core modules of TESTed and the language-specific modules.
No other modules have language-specific code.

The main task when adding support for a new programming language is to implement this abstract base class.
Some other smaller tasks are registering the language in TESTed and adding support in the test suite for this new programming language.

The remainder of this section describes the different methods that must or can be implemented.
We always begin by providing the signature, followed by a discussion of the method.
Most of this information is also available in the class itself as documentation in the code.

\subsection{Compilation}\label{subsec:impl-compilation}

\begin{minted}{python}
    def compilation(self, files: list[str]) -> CallbackResult:
\end{minted}

The compilation method must return the command that TESTed will use to compile the test code together with the submission.
The return type \mintinline{python}{CallbackResult} is an alias for \mintinline{python}{tuple[Command, list[str] | FileFilter]}.

The first value of the returned tuple is the compilation command.
This command is a list of strings, which will be executed with the Python \mintinline{python}{subprocess} package.
For generating this command, the \mintinline{python}{files} argument can be useful: it is a list of potential dependencies.
By convention, the last file in the list is the file containing the \emph{program entry point} (which is often a main function).

The second part of the return value must be a list of generated files, in which by convention the last file is the executable file.
All files in this list will be made available to the execution command in the next step of the evaluation process.
Alternatively, a file filter can be returned, which allows dynamic filtering of the resulting files after compilation.

As an example, consider the C language.
When compiling C, we are only interested in the resulting binary, which also has a predictable name.
The list of generated files can thus contain a single string: the name of the generated binary.

However, it is not always possile to predict the list of generated files, nor is it possible to predict their names.
For example, in Java, compiling a file will result in one or more class files, depending on the content of the Java file (a nested class will result in more class files).
In that case, the file filter can be used, which will be called for each file in the compilation directory after compilation has completed.

As a concrete example, this is how the method is called and what its return value is for C (on Windows):

\begin{minted}{pycon}
    >>> compilation(['submission.c', 'evaluation_result.c', 'context_0_0.c', 'selector.c'])
    (
        ['gcc', '-std=c11', '-Wall', 'evaluation_result.c', 'values.c', 'selector.c',
         '-o', 'selector.exe'], ['selector.exe']
    )
\end{minted}

The compilation method is optional: languages that do not require compilation can use the default implementation.
However, this step is ideal to at least perform a syntax check, and we recommend that all languages do this, if at all possible.
Even non-compiled languages often have a syntax checker that is faster than executing the program.
For example, both Python and JavaScript are not considered compiled languages, but both implement a syntax check in TESTed.

\subsection{Execution}\label{subsec:impl-execution}

One of the most important methods is the method responsible for creating the execution command.
This method is called after the compilation step, if that step was successful.

\begin{minted}{python}
    def execution(self, cwd: Path, file: str, arguments: list[str]) -> Command:
\end{minted}

This method must return one value: the command to execute.
As with the compilation method, the returned command will be executed by passing it to Python's \mintinline{python}{subprocess} package.

The returned command must execute the file from the \mintinline{python}{file} argument.
The argument \mintinline{python}{arguments} contains a list of command line arguments that must be passed to the program.
The \mintinline{python}{cwd} arguments is the directory in which the execution will take place.
This can be useful for languages that compile to a binary executable.
Since this executable is not on the path, it is safer to return an absolute path to it.

Continuing with the same example in C, a call to this method would look like this:

\begin{minted}{pycon}
    >>> execution('/test/path', 'executable.exe', ['arg1', 'arg2'])
    ['/test/path/executable.exe', 'arg1', 'arg2']
\end{minted}

All files that were included in the return value of the compilation method will also be available in the execution directory, in addition to other dependencies that we discuss next.

\subsection{Dependencies and other files}\label{subsec:dependencies-and-other-files}

In the commands from two previous sections, the methods receive a list of files that are potential dependencies for compilation or execution.
There are also some methods that optionally can influence which files are considered dependencies.

\begin{minted}{python}
    def initial_dependencies(self) -> list[str]:
\end{minted}

Returns a list of additional dependencies that will be included in the compilation and execution.
The returned strings are paths to files, relative to the implementation folder of the language module in TESTed.

For example, most languages include a separate file to deal with encoding return values into the TESTed data format.

\begin{minted}{python}
def filter_dependencies(self, files: list[Path], context_name: str) -> list[Path]:
\end{minted}

Used to filter the results of the compilation step to the files needed for one testcase.
In most cases, a single compilation step is used for all testcase.
However, not all languages need all resulting files for each execution.
By default, the name of the testcase is used to filter the files.

\begin{minted}{python}
def find_main_file(self, files: list[Path], name: str) -> tuple[Path | None, Status]
\end{minted}

This optional method finds the main file (i.e.\ the executable file or the file with the main method) in a list of dependencies.
The method should either return the path to that file with the status \texttt{CORRECT}, or return \mintinline{python}{None} with an error status if the file could not be found.
It can be useful if the normal convention of putting the main file last in a list of files is not enough.

\begin{minted}{python}
    def modify_solution(self, solution: Path):
\end{minted}

A callback that allows modifying the submission.
The submission should be modified in place.
The callback is called after linting, but before compilation or execution.

An example of this use case is JavaScript.
To support both CommonJS and ES6 modules, we analyze the code and add exports for all functions, variables, and classes in the submission.
Similarly, the main function in C programs is renamed to prevent conflicts with the main function in the generated TESTed code.

\subsection{Configuration and conventions}\label{subsec:configuration-and-conventions}

There are a number of simple methods that deal with the different conventions used by the programming language.

\begin{minted}{python}
    def get_string_quote(self):
\end{minted}

Returns the character used to quote strings.
By default, this is a double quotation mark (\texttt{"}).

\begin{minted}{python}
    def naming_conventions(self) -> dict[Conventionable, NamingConventions]:
\end{minted}

Returns the naming conventions used by the programming language.
It must return a dictionary, which maps different aspects to a naming convention.

The ``conventionable'' aspects are namespaces, function names, identifiers, properties, classes, and global identifiers.
Most are self-explanatory, except for namespace, whose meaning depends on the programming language.
In some languages this is used as the name for packages or modules, but in Bash, for example, it is used as the name of the script.
\Cref{tab:naming-conventions} contains an overview of the available naming conventions in TESTed.

\marginnote{
    In practice, most languages use camel case and snake case (and their uppercase variants).
}
\begin{table}[h]
    \centering
    \caption{Available naming conventions in TESTed.}
    \label{tab:naming-conventions}
    \begin{tabular}{|l|l|}
        \hline
        Naming convention & Example \\
        \hline
        Camel case & \texttt{thisIsAnExample}  \\
        Snake case & \texttt{this\_is\_an\_example} \\
        Camel snake case & \texttt{this\_Is\_An\_Example} \\
        Cobol case & \texttt{THIS-IS-AN-EXAMPLE} \\
        Dash case & \texttt{this-is-an-example} \\
        Donor case & \texttt{this|is|an|example} \\
        Flat case & \texttt{thisisanexample} \\
        Macro case & \texttt{THIS\_I\_AN\_EXAMPLE} \\
        Pascal case & \texttt{ThisIsAnExample} \\
        Pascal snake case & \texttt{This\_Is\_An\_Example} \\
        Train case & \texttt{This-Is-An-Example} \\
        Upper (flat) case & \texttt{THISISANEXAMPLE} \\
        \hline
    \end{tabular}
\end{table}

\begin{minted}{python}
    def file_extension(self) -> str:
\end{minted}

Return the main file extension for the programming language.
For language with multiple extensions, this should return the extension used for executable (main) files
For example, in C this returns \texttt{c} and not \texttt{h}.

\begin{minted}{python}
    def is_source_file(self, file: Path) -> bool:
\end{minted}

An optional method that determines if a file could be a source file for the programming language.
By default, this will check the extension of the file against the extension provided by the \mintinline{python}{file_extension} method.

\begin{minted}{python}
    def submission_file(self) -> str:
\end{minted}

Returns the name of the submission file.
By default, this calls the helper function \mintinline{python}{submission_name} and adds the file extension to it.

\subsection{Type support}\label{subsec:type-support}

The system for data types used by TESTed is explained in TODO.
The actual implementation of this system happens with the following methods.

\begin{minted}{python}
    def supported_constructs(self) -> set[Construct]:
\end{minted}

This method should return a set of the constructs that are supported by this language.
This is one of the mechanisms used in~\cref{subsec:solvability-and-correctness-checks} to check if a submissions in a certain programming language are possible for a given test suite.

The currently supported constructs are:

\begin{description}
    \item[Objects] Object-oriented constructs, such as classes.
    \item[Exceptions] Exception support.
    \item[Function calls] Function call support.
    \item[Assignments] The result of an expression can be assigned to a variable.
    \item[Heterogeneous collections] Data structures whose elements can be of different data types.
    \item[Default parameters] Parameters in a function with a default value.
    \item[Named arguments] Paramaters in a function can be passed by name rather than (or in addition to) by position.
    \item[Global variables] Variables or constants defined at a top-level.
\end{description}

Note that the constructs can sometimes be interpreted in a loose sense.
For example, Haskell indicates that assignments are supported, even if this is not strictly true.
\mintinline{haskell}{x = 5 + 5} defines a new function \mintinline{haskell}{x} with the body \mintinline{haskell}{5 + 5}.
However, for practical purposes, this can fulfill the same role as an assignment in TESTed.

\begin{minted}{python}
    def map_type_restrictions(self) -> set[ExpressionTypes] | None:
    def set_type_restrictions(self) -> set[ExpressionTypes] | None:
\end{minted}

These two optional functions allow restricting which data types are allowed in map keys and sets respectively.
For example, in Python, a list is not hashable, meaning it cannot be used as the key in a dictionary, nor can it be an element of a set.

\begin{minted}{python}
    def datatype_support(self) -> dict[AllTypes, TypeSupport]:
\end{minted}

This function is used to indicate the data type support for the language, as described in X.
The return value is a mapping of the types to their support.
The default is unsupported: only supported types must be present.

For example, in Bash, the implementation of this function looks like:

\begin{minted}{python}
    def datatype_support(self) -> dict[AllTypes, TypeSupport]:
        return {
            AdvancedStringTypes.CHAR: TypeSupport.REDUCED,
            BasicStringTypes.TEXT: TypeSupport.SUPPORTED,
        }
\end{minted}

Strings are supported (and are the only type supported by TESTed).
Characters are supported, but in reduced form.
This means that strings will be used for characters.

\subsection{Stacktraces and compiler outputs}\label{subsec:error-messages-and-compiler-outputs}

In most cases, stacktraces from errors, compiler errors, or compiler warnings contain lines referring to the generated TESTed code.
However, these lines are not relevant nor useful to students.
Therefore, these methods provide a way to clean up stacktraces.

Another example of a use-case for these is adding links in the feedback to the relevant lines in the submission.
Dodona also supports this feature, so users there can click on a stacktrace and go to the relevant lines, like some IDEs.

\begin{minted}{python}
    def compiler_output(self, stdout: str, stderr: str) -> tuple[list[Message], list[AnnotateCode], str, str]
\end{minted}

Allows cleaning up the output from the compiler.
The returned tuple contains a list of messages, a list of code annotations, and the clean version of the compiler output.

\begin{minted}{python}
    def cleanup_stacktrace(self, stacktrace: str) -> str:
\end{minted}

Cleans up a stacktrace.

\subsection{Code generation}\label{subsec:code-generation2}

As mentioned in \cref{subsec:code-generation}, TESTed works by generating code.
The following methods are called on the language module to generate code for various language constructs.
In the implementation for most languages, the code generation is implemented in a separate module, and these methods just call that module.

\begin{minted}{python}
    def generate_statement(self, statement: Statement) -> str
\end{minted}

Generate code for a statement.

\begin{minted}{python}
    def generate_execution_unit(self, execution_unit: "PreparedExecutionUnit") -> str:
\end{minted}

Generate code for an execution unit.
Of course, it is expected that the implementation of this method uses the other methods for generating code.
For example, an execution unit probably needs to generate code for a statement somewhere, which would be done using \mintinline{python}{generate_statement}.

When generating the code for an execution unit, a few things must be taken into account (\cref{fig:generated-code}):

TODO: namen...
\begin{itemize}
    \item TESTed expects two secret sentinel values to be present in all outputs (stdout, stderr, return values, exceptions): \texttt{testcase\_separator\_secret} between the outputs for tests and \texttt{context\_separator\_secret} between the outputs for testcases.
    \item Return values and exceptions must be serialized in JSON, using the internal data form from TESTed.
    \item The generated code should be robust against unexpected output, including exceptions.
\end{itemize}

\begin{figure}
    \centering
    \includestandalone{code-generation}
    \caption{Overview of code generation in TESTed.
        In the optimal scenario, a single executable is generated and compiled for the whole test suite.
        Its main function has one job: selecting the correct function for the relevant execution unit.
        This is illustrated here with a \mintinline{java}{switch}.
        Each of these execution functions will write a separator to all outputs (return values, stdout, stderr, exceptions) and will then call the function for the contexts in that execution unit.
        This function for the context does something very similar: it writes the context separator to the outputs and execute the testcases.
        The final results for each output are a set of captured values separated by the context and testcase separators.
    }
    \label{fig:generated-code}
\end{figure}


\section{\tested{} and Dodona}\label{sec:tested-and-dodona}

Dodona (TODO: cite) is an online platform for solving and submitting programming exercises.
It is developed by Team Dodona (of which I am a part) at Ghent University, originally for use in our own courses.
However, the platform has enjoyed a wide adoption within Flanders, and is now used by over a thousand colleges, universities, and schools.
While TESTed is available as a standalone tool, it was primarily developed with Dodona in mind.
Therefore, some design decisions made by the Dodona platform still apply to TESTed as well.

In Section TODO, we have already made a distinction between a platform and the judge.
Some other platforms (and accompanying judges) are discussed in TODO.

In this section, we briefly discuss the relevant aspects of Dodona that have an impact on TESTed.
We also look at Dodona's feedback format, which is also used by TESTed.

\subsection{Architecture}

A relatively unique aspect of Dodona is the complete separation of the platform code and the judge code.
The platform code is the web application and responsible for almost everything, from classroom management, to rendering web pages with the user interface used by students and teachers.
The judge is responsible for generating the feedback for a submission.

Communication between the platform and the judges is done via a well-defined API, which are discussed in the next two sections.
As judges run student code, they must be particularly immune to bad code.
For example, a submission with an infinite loop should not bring down the platform.
Neither should malevolent submissions: a fork bomb should similarly have no impact on the platform.

To achieve this, judges are executed in Docker (TODO).
This ensures the judges are isolated from the platform and allows us to easily impose resource limits on the execution of submissions (such as memory limits, time limits, disk speed limits, network access or not).

When code is submitted, the platform will create a Docker container using the judge's associated Docker image.
Then, relevant files for the exercise and the submission are mounted into the container's file system.
Finally, the container is run, which will start the judge inside the container with relevant options (\cref{subsec:dodona-input}).
Dodona then reads the feedback from the standard output of the container (\cref{subsec:dodona-output}).
This feedback is then saved into Dodona's database and shown to the user who submitted the code.

\subsection{Input}
\label{subsec:dodona-input}

When a judge is started by Dodona, a JSON object is provided on standard input.
This configuration object contains all information needed by the judge to evaluate a submission.
An annotated example is provided in~\cref{lst:dodona-input-object}.

\begin{listing}
    \begin{minted}{json}
{
  // The programming language of the submission.
  "programming_language": "python",
  // The natural language used when submitting.
  "natural_language": "en",
  // Path to the resource folder of the exercise.
  "resources": "/exercise/simple-example/",
  // Path to the submission's source code.
  "source": "/exercise/simple-example/correct.py",
  // Path to the judge.
  "judge": "/tested/",
  // Path the a workdir, where execution should happen.
  "workdir": "/temp/workdir/",
  // Memory limit, in bytes.
  "memory_limit": 536870912,
  // Time limit, in seconds.
  "time_limit": 60,
  // Name of the test suite.
  "suite": "suite.yaml",
  // Tested: additional options...
  "linter": true
}
    \end{minted}
    \caption{
        Annotated example of the input provided to judges by Dodona.
        This is also the input expected by TESTed.
        This object is provided on standard input to the judge when started by Dodona.
    }
    \label{lst:dodona-input-object}
\end{listing}

Most of the options are pretty straightforward.
Of the generic options, the memory and time limit are informational: they are provided so that a judge can make an effort to limit submissions.
However, Dodona will enforce these limits if the limits are exceeded.
A common use-case is to provide better or more detailed feedback about the issue (since Dodona, by design, can only indicate a global memory or time limit exceeded error).

Additional judge-specific options are also added to this object.
For example, with TESTed, the name of the test suite is often such an option.
Other usable options for TESTed are\footnote{These are also described in our documentation at \url{https://docs.dodona.be/en/references/tested/exercise-config/}}:

\begin{description}
    \item[\texttt{parallel}] If contexts should be executed in parallel or not (default false). See \cref{subsec:executing-test-code} for more information on this option.
    \item[\texttt{allow\_fallback}] Determines if unit compilation should be attempted if the global compilation fails (default true). See \cref{subsubsec:compilation-units} for more information about this option.
    \item[\texttt{linter}] Enables or disables linting of the submissions (default true).
    \item[\texttt{language}] An object mapping programming languages to objects containing language-specific options. As an example use-case of this option, some languages allow customizing the linter configuration. TODO: link
    \item[\texttt{compiler\_optimizations}] If compiler optimizations should be enabeld if available or not (default false). By enabling this option, compile speed is sacrificed for better execution speed of the submission. This option can be useful for exercises where the submission is expected to be computationally heavy.
\end{description}

\subsection{Output (the feedback format)}
\label{subsec:dodona-output}

Dodona supports two output formats: a \emph{full} and a \emph{partial} output format.
TESTed uses the partial output format, so we will only discuss that format.

The feedback has a very similar structure as the test suite (discussed in \cref{sec:test-suite-structure}).
The format is named partial since it is a streaming JSON format.
This means that the output is a stream of JSON objects, instead of one big JSON object.

\marginnote{There also exists RFC 7464~\cite{rfc7464}, where each JSON object is started by a \emph{record seperator} character and ended with a newline. This is a relatively new format that doesn't seem to be widely-used.}
TODO: cite this?.
TESTed uses newline-delimited JSON\@.
Two equivalent \emph{specifications} exist for this format: NDJSON (Newline-Delimited JSON\footnote{\url{https://ndjson.org/}}) and JSON Lines\footnote{https://jsonlines.org/}.
The format itself is simple: JSON objects are separated by a newline, and each line is a valid JSON object.

The structure of the feedback is indicated by commands, with \emph{start} commands to begin a new level in the hierarchy and \emph{close} commands to finish a level.
\cref{lst:tested-output-example} contains the output from TESTed that resulted in the feedback as shown in~\cref{fig:dodona}.

\begin{listing}
    \begin{minted}{json}
        {"command": "start-judgement"}
        {"command": "start-tab", "title": "Feedback"}
        {"command": "start-context"}
        {"command": "start-testcase", "description": "a_list = [1, 2, 3, 2]"}
        {"command": "close-testcase"}
        {"command": "start-testcase", "remove_all_occurrences(a_list, 2)"}
        {"command": "start-test", "expected": "[1, 3]", "channel": "return"}
        {"command": "close-test", "generated": "[1, 3]", "status": "correct"}
        {"command": "close-testcase"}
        {"command": "close-context"}
        {"command": "start-context"}
        {"command": "start-testcase", "description": "a_list = [0, 1, 1, 2]"}
        {"command": "close-testcase"}
        {"command": "start-testcase", "remove_all_occurrences(a_list, 1)"}
        {"command": "start-test", "expected": "[0, 2]", "channel": "return"}
        {"command": "close-test", "generated": "[0, 2]", "status": "correct"}
        {"command": "close-testcase"}
        {"command": "close-context"}
        {"command": "close-tab"}
        {"command": "close-judgement"}
    \end{minted}
    \caption{
        Example of the output generated by TESTed, which is rendered in \cref{fig:dodona}.
        Note that the names of the levels are the Dodona levels (tab, contex and testcase), instead of unit, testcase and script.
        As before, each testcase consists of two tests, the first of which has no script, while
        the second has one script (the expected return value).
    }
    \label{lst:tested-output-example}
\end{listing}

The Dodona feedback format is a simple, yet flexible format.
It has been used by a variety of judges for general purpose programming languages (like TESTed, but also dedicated judges for JavaScript, Bash, Python, C, C\#, Prolog, Haskell, R, and Scheme).
It has also been used successfully for more niche judges (such as judges for \textsc{HTML}/\textsc{CSS}, \textsc{SQL}, and Turtle).
This illustrates that TESTed is neither limited by this choice of output format, nor would it be challenging to support this format in other platforms.


\end{document}
