\documentclass[main]{subfiles}

\begin{document}

\chapter{Conclusions and opportunities}\label{ch:conclusions-and-opportunities}

As an answer to the four research questions mentioned in \cref{sec:a-note-on-the-structure-of-this-thesis},
we introduced five educational tools to facilitate programming education:
two for textual programming languages and three for block-based programming languages.

\section{Textual programming languages}\label{sec:for-textual-programming-languages}

Our contributions make it possible to ergonomically create a single programming exercise, with one test suite and one task description, and use this exercise in multiple programming languages.
To make this possible, we created TESTed and TESTed-DSL, which we discuss below.

\begin{description}
    \item[RQ1] Can we design an educational software testing framework that supports automated assessment across programming languages based on a single test suite?
\end{description}

We can, as we demonstrate by presenting our implementation of such a testing framework: \textbf{TESTed}.
First, we identified input/output testing and unit testing as two opposing strategies commonly used in educational software testing.
Our initial investigation focused on understanding how these approaches affect the supported programming languages within the testing frameworks.
Often, testing frameworks that fall under input/output testing support multiple programming languages, but the quality of the feedback suffers.
On the other hand, frameworks using unit testing have much more fine-grained feedback, but only support a single programming language.

Our aim was to combine the best of both worlds.
To this end, we formulated the requirements for programming-language-agnostic testing frameworks that combine unit testing with support for multiple programming languages.
We then introduced TESTed, and detailed its internal workings.
Finally, we evaluated TESTed in educational practice to verify that it supports our requirements for a programming-language-agnostic testing framework.

We see opportunities for more work on TESTed in the future.
Our goal is to further develop TESTed for authoring different types of programming exercises across programming languages.
TESTed is currently focused mainly on dynamic testing.
A key area of future interest is the implementation of language-agnostic static code analysis capabilities.

\begin{description}
    \item[RQ2] What is the most ergonomic way to author programming exercises with support for automated assessment across programming languages?
\end{description}

We concluded that a domain-specific language, designed specifically for this purpose, is the best approach.
We then introduced our implementation: \textbf{TESTed-DSL}\@.

We again first looked at input/output testing and unit testing as the two opposing strategies.
However, this time, we focused more on the impact of these strategies on the testing process itself.
We considered what can be tested and how, in addition to how and what feedback is generated.

The conclusion was again that the best of both strategies provides the best experience for educators.
Looking at programming-language-agnostic testing frameworks more broadly, we reported on three benefits for the adoption of such frameworks: \begin{enumerate*}[label=\emph{\roman*})] \item sharing the same declarative structure across programming languages, \item bridging the gap between input/output testing and unit testing, and \item allowing test code to be expressed in a language-agnostic way.\end{enumerate*}

We also see potential for additions to TESTed-DSL in the future.
These include supporting operator overloading, string conversion, comments, indexing sequences, indexing mappings, destructuring, object identity checking, and object equivalence checking.
Native support for pretty printing nested data structures would be another valuable addition, making it easier to detect differences between expected and actual return values.
There are more opportunities still, including data-driven tests (parameterized tests), supporting dynamic generation of test data and boosting the performance of running tests.

\section{Block-based programming languages}\label{sec:for-block-based-programming-languages}

For Scratch, our contributions make it possible to automatically assess Scratch exercises, providing students with qualitative and timely formative feedback.
For this purpose, we created Itch, an educational testing framework for Scratch.
Based on the feedback students receive from Itch, they can then use Blink, the debugger we created, to find the root cause of their issues.

\begin{description}
    \item[RQ3] Can we design an educational software testing framework for the block-based programming language Scratch?
\end{description}

Yes, as shown by our implementation of such a framework: \textbf{Itch}.
We showed that it offers a versatile approach to testing, allowing static testing, emulating user interaction, and performing post-mortem testing.
Tests can range from purely static to purely dynamic, or a hybrid of both.

However, we also reported that while most exercises can be tested, it remains difficult to design Scratch exercises that are both dynamically testable and sufficiently open-ended to align with the game-like and exploratory nature of Scratch.
Static tests, though faster and sometimes easier to write, can potentially constrain creativity and go against the spirit of Scratch.

We also found that educators that are primarily experienced in Scratch might find JavaScript test suites difficult to write.
For this reason, we created and reported on a prototype of a Scratch-based testing framework called \textbf{Poke}.
It allows creating test suites with Scratch, using the blocks and environment Scratch users are familiar with.
While writing the tests is technically feasible, some challenges remain, the main one being the organizational aspects of managing these Scratch tests suites.

\begin{description}
    \item[RQ4] Can we design an (educational) debugger for the block-based programming language Scratch?
\end{description}

Yes: \textbf{Blink} is our time-travelling debugger for Scratch.
Working with Itch in an educational setting made clear that testing frameworks primarily indicate whether a submission is correct or not, but do not directly assist students in finding the root cause of a failed test.

To address this, we developed Blink, a debugger for Scratch.
Debuggers are generally known to be good tools for finding errors in a program, and this is no different in Scratch.
Blink supports pausing execution, stepping through code, and provides time travel capabilities.
We prioritized the user-friendliness of Blink, given Scratch's younger target audience.
Due to the concurrent nature of Scratch, we had to hide a lot of the complexities of concurrent debugging for the user.
Initial feedback from using Blink in a classroom setting has been positive: pupils find the debugger intuitive and useful, especially the time-travelling capability.

One of the ways we sought to make the debugger more intuitive was to use a non-traditional stepping functionality.
In most debuggers, a step will advance the code one step in a single thread.
However, in Scratch, we wanted the step to advance one step in all threads simultaneously.
The current execution model makes this difficult.

We first explained in detail how the current execution model behaves.
The current execution model has been chosen to minimize the occurrence of some concurrency-related issues, like race conditions, but does not prevent all issues.
The threading model, in particular, causes some surprising behaviour, which is not ideal as Scratch is intended to be intuitive.

Next, we proposed changes to the execution model of Scratch that seek to resolve these issues.
However, the widespread use of Scratch demands that any changes must not adversely affect existing projects.
For this reason, we explored how Scratch is used, the results of which corroborate previous findings: most Scratch projects are small.

To conclude, we benchmarked the execution model on various representative projects to measure its real-world impact.
While there is some impact, we deem it acceptable.
This strengthens our confidence in integrating the modified execution model not only with the debugger, but in general use of Scratch.

In the future, we envision integrating Itch with Blink.
In an ideal scenario, a failed test from Itch would allow students to directly open a debugging session when the test failed.
Using the time-travelling features of Blink, students can then go back in time until they find the issue.

\end{document}
