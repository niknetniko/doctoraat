\documentclass[./main]{subfiles}

\begin{document}

\chapter{Introduction}\label{ch:introduction}

Learning to program is hard, and students consequently regard programming courses as difficult~\autocite{robinsLearningTeachingProgramming2003,simoesNatureProgrammingExercises2020}.
It is generally accepted that gaining a deep understanding of programming requires experience and feedback~\autocite{gomesEnvironmentImproveProgramming2007}.
This feedback is what makes teaching programming difficult: it is a very time-consuming task to provide good feedback on code, especially if the number of students in a course is high~\autocite{zavalaUseSemanticbasedAIG2018,staubitzRepositoryOpenAutogradable2017,queirosPexilProgrammingExercises2011,pirttinenCrowdsourcingProgrammingAssignments2018,gulwaniFeedbackGenerationPerformance2014,tangDatadrivenTestCase2016}.

\section{Origins and use of automated assessment in computer science education}\label{sec:automated-assessment-in-computer-science-education}

Education of concepts that we would now consider computer science goes back as early as the 1940s.
A good overview on the history of computer science education is given in \textcite{tedreChangingAimsComputing2018}, to which we point the readers for a complete overview.
Of particular interest to this thesis is the education of programming, which began appearing curricula in the early 1960s~\autocite{simonEmergenceComputingEducation2015}.
This is a bit after when that computer science became recognised as its own scientific discipline in the 1950s and 1960s~\autocite{hopcroftComputerScienceEmergence1987,atchisonComputerScienceNew1971,gornComputerInformationSciences1963,knuthComputerScienceIts1974,denningScienceComputerScience2013}.

From the very start of programming education, the need for automated assessment was noted.
Generally considered to be the first publication on automated assessment, \textcite{hollingsworthAutomaticGradersProgramming1960} details their testing framework and remarks that they could not accommodate the number of students in their programming classes without their ``automatic grader''.
Since then, there has been a long history of using automated assessment~\autocite{ala-mutkaSurveyAutomatedAssessment2005,douceAutomaticTestbasedAssessment2005,ihantolaReviewRecentSystems2010,paivaAutomatedAssessmentComputer2022,combefisAutomatedCodeAssessment2022,nayakAutomatedAssessmentTools2022,messerAutomatedGradingFeedback2024}.

The most basic functionality of these frameworks is to act as an (educational) software testing framework.
They determine if a particular submission is correct, based on a set of tests (the test suite) provided by the educators who created the programming exercise.
There is also overlap with the automated assessment in programming contests, where the frameworks are often called judges.
Since both automated assessment frameworks and judges do, in essence, the same thing, it is not surprising that they influence each other or that some frameworks are used in both contexts.

\section{The Dodona platform}\label{sec:dodona}

As in most programming courses, the ones taught by our departement also used automated assessment.
\marginnote{As a student, I was one of the last years to uses SPOJ, altough that statement does make me feel old.}
This was done using the Sphere Online Judge (or SPOJ)~\autocite{kosowskiApplicationOnlineJudge2007}, which was relatively unique in the fact that it allowed teachers to create their own courses, exercises, and even testing frameworks.

In 2016, the needs for these courses outgrew the Sphere Online Judge, which led to the creation of our own platform: Dodona~\autocite{vanpetegemDodonaLearnCode2023}.
Dodona is now used by most higher education institutions in Flanders and by many schools providing secondary education.
As of May 2024, Dodona has more than \num{69200} registered users.

Dodona has been developed with support for multiple programming languages from the start.
It has a strict separation between the platform (responsible for managing courses, students, and exercises) and the testing framework (determining if a submission is correct).
This separation has allowed for easily supporting a multitude of programming languages.

Within Dodona, the testing framework is called a judge (and evaluating a submission for correctness is called judging).
At a very high level, an evaluation of a submission goes through the following process in Dodona: first, a Docker image containing the judge is started, the submission and other needed information (like the test suite) is provided to the Docker image via standard input, and Dodona expects the feedback for the students to appear on standard output.

While the earlier Dodona judges were for Python and JavaScript, its interface for judge proved to be generic enough that it can support a multitude of scenarios.
For example, there are currently judges for C, Haskell, Java, Prolog, R, Scheme, Bash, C\#, JavaScript, and Python (and also TESTed of course).
There are also a few less straightforward judges, such as HTML, SQL, Markdown, and Turtle.

\section{The first part: TESTed}\label{sec:intro-tested}

\subsection{Exercises for multiple programming languages}\label{subsec:exercises-for-multiple-programming-languages}

The support for multiple programming languages in Dodona led to the observation that a lot of programming exercises are usable in multiple programming languages.
However, to use an exercise in another programming language, first a hard copy of the exercise had to be made.
Then the test suite had to be converted manually to the test suite format used by the judge for that particular language.

Additionally, implementing the different judges was also repetitive for most programming languages.
For each judge, a new test suite format had to be created, and all other tasks that a judge does (e.g.\ test scheduling, output handling) had to be re-implemented for each new judge.
Additionally, since the exercises are copied, the exercises have to be kept in sync manually, or they risk diverging.

\subsection{Programming-language-agnostic testing}\label{subsec:programming-language-agnostic-testing}

This led to the creating of TESTed, a programming-language-independent testing framework.
One of its defining features is the ability to create programming-language-agnostic exercises.
This means that the same exercise (with one test suite) can be solved in multiple programming languages, with support for automated assessment.
Educators can create a programming exercise with a single test suite, which can then be solved in all programming languages supported by TESTed.

The research on TESTed began in 2019 with a master's thesis~\autocite{strijbolTESTedOneJudge2020}.
As with every new research, we began by experimenting with various techniques.
For example, very early versions of TESTed used Jupyter Kernels\footnote{\url{https://docs.jupyter.org/en/latest/projects/kernels.html}} as a mechanism to provide support for multiple programming languages.
However, due to speed concerns, this was quickly dropped in favour of executing custom code.

\Cref{ch:tested1} explains TESTed in more detail.
The current version uses custom code generation: we found that is a good trade-off between speed and ease of writing.

\subsection{Ergonomic testing}\label{subsec:ergonomic-testing}

Next, we began investigating the ergonomics of creating programming exercises~\autocite{selsTESTedProgrammeertaalonafhankelijkTesten2021}, which resulted in TESTed-DSL (\cref{ch:tested-dsl}).
This is a domain-specific language for authoring programming-language-agnostic exercises with support for automated assessment.
We also apply TESTed-DSL to task descriptions.

At this point, we came to the conclusion that TESTed was not only useful for creating programming-language-independent exercises.
As special care has been taken to design TESTed-DSL to be useful for a wide audience of educators, including higher and secondary education.
For this reason, we also invested in our documentation, which contains reference documentation and a set of tutorials for commonly used exercise types.

\section{The second part: Scratch}\label{sec:the-second-part:-scratch}

\subsection{Testing Scratch code}\label{subsec:testing-scrath-code}

Learning to program for young children is often done with specialised programming languages.
The most-used language in this context is Scratch (\cref{ch:scratch-the-programming-environment}), a visual block-based programming language~\autocite{resnickScratchProgrammingAll2009}.

Since Dodona is independent of the testing framework, we began by adding a testing framework for Scratch to Dodona~\autocite{makItchEenEducatief2019}.
We created a new framework since there has been relatively little existing work in this area.

However, Scratch is not only a programming language, but a complete programming environment.
We realised that Scratch required accommodations that were too different from what Dodona can provide.
Additionally, while our group had experience with programming exercises for text-based languages, we had much less experience with Scratch in an educational context.

To address these issues, we sought a partner in the industry: CodeCosmos, an educational publisher.
While we create the testing framework, they integrate it into their platform and use it with students and pupils.

The testing framework is described in \cref{ch:itch}.
Working with our partner, it did become clear that the JavaScript test suites for Itch were a big hurdle for educators without a computer science background.
As a lot of software testing frameworks are written in the same programming language as the code they test, we also investigated this possibility in Scratch~\autocite{voetenEenBlokgebaseerdTestframework2023}.
The result of this was Poke, a prototype of a testing framework for Scratch with test suites written in Scratch (\cref{sec:poke:-a-testing-framework-written-in-scratch}).

\subsection{Debugging Scratch code}\label{subsec:debugging-scratch-code}

While the goal of software testing itself is to find errors in a submission, we believe the ultimate goal is to help students write more correct programs.
So, the next step after a failed test is debugging the submission.
This is a two-step process~\autocite{myersArtSoftwareTesting2012}.
Step one is to determine the exact nature and location of the error.
Step two is then fixing the error.
It is well known that determining the cause of an observed failure is challenging~\autocite{ammannIntroductionSoftwareTesting2016}.
The debugging process is difficult, especially for novice programmers~\autocite{mccauleyDebuggingReviewLiterature2008}.
This has led to the creation of various tools to help with this, chief among them the debuggers~\autocite{rosenbergHowDebuggersWork1996}.

Since debuggers are also a less explored area of research in regard to Scratch, we started looking into creating a new debugger for Scratch~\autocite{deproftBlinkEenEducatieve2022,goethalsEenTimeTravelling2023}.
Based on the work of these two master's theses, we created a debugger for Scratch, named Blink, presented in \cref{ch:blink}.

Our work on Blink has revealed some noteworthy details of the Scratch execution model, which we believe to be suboptimal for a debugger.
We therefore propose and investigate some changes to this model in \cref{ch:scratch-execution-model}.
The aim is to verify if the proposed changes do not cause problems for existing Scratch projects, while still being an improvement for debuggers.

\section{The structure of this thesis}\label{sec:a-note-on-the-structure-of-this-thesis}

The root motivation for every part of this thesis is the observation that there exists a gap in existing educational tools.
We then set out to propose tools that fill this gap.
While we did not intend to split the textual and block-based tools initially, it quickly became clear that the needs for both, in addition to the existing research, were different enough to warrant a separate approach.

This is reflected in this thesis: the first part (\cref{ch:tested1,ch:tested-dsl}) is about textual programming languages, TESTed and TESTed-DSL\@.
The second part (\cref{ch:scratch-the-programming-environment,ch:itch,ch:blink,ch:scratch-execution-model}) is about Scratch-related tools (Itch, Blink, and the execution model).

While this introductory chapter gives a high-level overview of the thesis and the context in which it came to be, each part also has a preface with some more details, such as existing publications for the chapters.

\end{document}
